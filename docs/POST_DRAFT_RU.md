# Драфт поста (короткая версия)

Долго страдал от задержки транскрибации в локальном Whisper на длинных диктовках.

Сделали публичный кейс на `WhisperMac`: ускорили пайплайн без потери качества и без облачных API.

Что внедрили:
- incremental streaming вместо повторной конкатенации всего буфера;
- backlog flush после stop;
- adaptive final pass (только когда реально нужен);
- фильтр тишины/галлюцинаций;
- perf-метрики (`RTF`) в лог.

Что важно:
- решение полностью локальное;
- API-ключи не нужны;
- есть strict local/offline режим после первого кэша модели;
- можно отключить сохранение транскриптов на диск.

Собрали всё “по GitHub-канону”:
- README с быстрым стартом;
- SECURITY с прозрачной моделью данных;
- preflight-скрипт перед публикацией, чтобы не утащить секреты.

Если нужно, выложу в open-source с инструкцией “установил и поехали” для macOS.

Автор материала: [t.me/ei_ai_channel](https://t.me/ei_ai_channel)
